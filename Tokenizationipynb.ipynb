{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "VdEg1-THPxWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts=[\"I love deep learning\",\"RNNs are powerful for sequence data\",\"Tokenization is the first step\"]"
      ],
      "metadata": {
        "id": "emqT9SZeOtHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create tokenizer\n",
        "tokenzier=Tokenizer()\n",
        "#fit the tokenizer on the text\n",
        "tokenzier.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "vw-DqiRZPNLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert text to sequence\n",
        "sequenece=tokenzier.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "Xd9H3JYrPSvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pad the sequence to make the equal length\n",
        "padded_sequences=pad_sequences(sequenece,padding='post')"
      ],
      "metadata": {
        "id": "RYDtB2vmQQhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word Index:\\h\",tokenzier.word_index)\n",
        "print(\"\\nSequences:\\h\",sequenece)\n",
        "print(\"\\nPadded Sequences:\\h\",padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73nH-TMRQU49",
        "outputId": "a9ca9862-73b1-4183-8a32-1b3bf355935c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:\\h {'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'rnns': 5, 'are': 6, 'powerful': 7, 'for': 8, 'sequence': 9, 'data': 10, 'tokenization': 11, 'is': 12, 'the': 13, 'first': 14, 'step': 15}\n",
            "\n",
            "Sequences:\\h [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n",
            "\n",
            "Padded Sequences:\\h [[ 1  2  3  4  0  0]\n",
            " [ 5  6  7  8  9 10]\n",
            " [11 12 13 14 15  0]]\n"
          ]
        }
      ]
    }
  ]
}